{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34bed795-6159-4f89-8d40-721b03b6d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fd21668-09aa-475f-924b-bbacb1268ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text analytics is the process of extracting meaningful insights from textual data. It involves various preprocessing techniques such as tokenization, stop-word removal, stemming, and lemmatization. These techniques help in structuring unstructured text, making it easier to analyze.\n",
      "\n",
      "Natural Language Processing (NLP) plays a significant role in text analytics by enabling machines to understand and process human language. Applications of text analytics include sentiment analysis, document classification, and information retrieval.\n",
      "\n",
      "By computing Term Frequency-Inverse Document Frequency (TF-IDF), we can identify important words in a document relative to a collection of documents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"document.txt\", \"r\", encoding=\"utf-8-sig\") as file:\n",
    "    document = file.read()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2b13197-e054-4063-9591-d09c19ebb11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2727ccfe-5483-40f5-a5fb-3f47060538df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shreenath/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/shreenath/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shreenath/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/shreenath/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/shreenath/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/shreenath/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "197ef978-af9e-4dbb-9cb1-bb54afb64e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after Stop Word Removal:\n",
      "\n",
      " ['Text', 'analytics', 'process', 'extracting', 'meaningful', 'insights', 'textual', 'data', '.', 'involves', 'various', 'preprocessing', 'techniques', 'tokenization', ',', 'stop-word', 'removal', ',', 'stemming', ',', 'lemmatization', '.', 'techniques', 'help', 'structuring', 'unstructured', 'text', ',', 'making', 'easier', 'analyze', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'plays', 'significant', 'role', 'text', 'analytics', 'enabling', 'machines', 'understand', 'process', 'human', 'language', '.', 'Applications', 'text', 'analytics', 'include', 'sentiment', 'analysis', ',', 'document', 'classification', ',', 'information', 'retrieval', '.', 'computing', 'Term', 'Frequency-Inverse', 'Document', 'Frequency', '(', 'TF-IDF', ')', ',', 'identify', 'important', 'words', 'document', 'relative', 'collection', 'documents', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(document)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"\\nTokens after Stop Word Removal:\\n\\n\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "293508c2-cd5d-42db-abb0-150973e405fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame (Stemmed Sentences):\n",
      "\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    | Original Sentence                                                                                                                                     | Stemmed Sentence                                                                                                                        |\n",
      "+====+=======================================================================================================================================================+=========================================================================================================================================+\n",
      "|  0 | Text analytics is the process of extracting meaningful insights from textual data.                                                                    | text analyt is the process of extract meaning insight from textual data .                                                               |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  1 | It involves various preprocessing techniques such as tokenization, stop-word removal, stemming, and lemmatization.                                    | it involv variou preprocess techniqu such as token , stop-word remov , stem , and lemmat .                                              |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  2 | These techniques help in structuring unstructured text, making it easier to analyze.                                                                  | these techniqu help in structur unstructur text , make it easier to analyz .                                                            |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  3 | Natural Language Processing (NLP) plays a significant role in text analytics by enabling machines to understand and process human language.           | natur languag process ( nlp ) play a signific role in text analyt by enabl machin to understand and process human languag .             |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  4 | Applications of text analytics include sentiment analysis, document classification, and information retrieval.                                        | applic of text analyt includ sentiment analysi , document classif , and inform retriev .                                                |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  5 | By computing Term Frequency-Inverse Document Frequency (TF-IDF), we can identify important words in a document relative to a collection of documents. | by comput term frequency-invers document frequenc ( tf-idf ) , we can identifi import word in a document rel to a collect of document . |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from tabulate import tabulate\n",
    "\n",
    "sentences = sent_tokenize(document)\n",
    "\n",
    "df = pd.DataFrame({'Original Sentence': sentences})\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])\n",
    "\n",
    "df['Stemmed Sentence'] = df['Original Sentence'].apply(stem_words)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"\\nProcessed DataFrame (Stemmed Sentences):\\n\")\n",
    "print(tabulate(df, headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc71e81-922c-428d-8a62-f1a79ed688c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame (Lemmatized Sentences):\n",
      "\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    | Original Sentence                                                                                                                                     | Lemmatized Sentence                                                                                                                                     |\n",
      "+====+=======================================================================================================================================================+=========================================================================================================================================================+\n",
      "|  0 | Text analytics is the process of extracting meaningful insights from textual data.                                                                    | Text analytics is the process of extracting meaningful insight from textual data .                                                                      |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  1 | It involves various preprocessing techniques such as tokenization, stop-word removal, stemming, and lemmatization.                                    | It involves various preprocessing technique such a tokenization , stop-word removal , stemming , and lemmatization .                                    |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  2 | These techniques help in structuring unstructured text, making it easier to analyze.                                                                  | These technique help in structuring unstructured text , making it easier to analyze .                                                                   |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  3 | Natural Language Processing (NLP) plays a significant role in text analytics by enabling machines to understand and process human language.           | Natural Language Processing ( NLP ) play a significant role in text analytics by enabling machine to understand and process human language .            |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  4 | Applications of text analytics include sentiment analysis, document classification, and information retrieval.                                        | Applications of text analytics include sentiment analysis , document classification , and information retrieval .                                       |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|  5 | By computing Term Frequency-Inverse Document Frequency (TF-IDF), we can identify important words in a document relative to a collection of documents. | By computing Term Frequency-Inverse Document Frequency ( TF-IDF ) , we can identify important word in a document relative to a collection of document . |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from tabulate import tabulate\n",
    "\n",
    "sentences = sent_tokenize(document)\n",
    "\n",
    "df = pd.DataFrame({'Original Sentence': sentences})\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "df['Lemmatized Sentence'] = df['Original Sentence'].apply(lemmatize_words)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"\\nProcessed DataFrame (Lemmatized Sentences):\\n\")\n",
    "print(tabulate(df, headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afb8beed-5528-4065-a9ff-6451b5e829cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame:\n",
      "\n",
      "      tokenized_word:\n",
      "0                Text\n",
      "1           analytics\n",
      "2                  is\n",
      "3                 the\n",
      "4             process\n",
      "5                  of\n",
      "6          extracting\n",
      "7          meaningful\n",
      "8            insights\n",
      "9                from\n",
      "10            textual\n",
      "11               data\n",
      "12                 It\n",
      "13           involves\n",
      "14            various\n",
      "15      preprocessing\n",
      "16         techniques\n",
      "17               such\n",
      "18                 as\n",
      "19       tokenization\n",
      "20          stop-word\n",
      "21            removal\n",
      "22           stemming\n",
      "23                and\n",
      "24      lemmatization\n",
      "25              These\n",
      "26         techniques\n",
      "27               help\n",
      "28                 in\n",
      "29        structuring\n",
      "30       unstructured\n",
      "31               text\n",
      "32             making\n",
      "33                 it\n",
      "34             easier\n",
      "35                 to\n",
      "36            analyze\n",
      "37            Natural\n",
      "38           Language\n",
      "39         Processing\n",
      "40                NLP\n",
      "41              plays\n",
      "42                  a\n",
      "43        significant\n",
      "44               role\n",
      "45                 in\n",
      "46               text\n",
      "47          analytics\n",
      "48                 by\n",
      "49           enabling\n",
      "50           machines\n",
      "51                 to\n",
      "52         understand\n",
      "53                and\n",
      "54            process\n",
      "55              human\n",
      "56           language\n",
      "57       Applications\n",
      "58                 of\n",
      "59               text\n",
      "60          analytics\n",
      "61            include\n",
      "62          sentiment\n",
      "63           analysis\n",
      "64           document\n",
      "65     classification\n",
      "66                and\n",
      "67        information\n",
      "68          retrieval\n",
      "69                 By\n",
      "70          computing\n",
      "71               Term\n",
      "72  Frequency-Inverse\n",
      "73           Document\n",
      "74          Frequency\n",
      "75             TF-IDF\n",
      "76                 we\n",
      "77                can\n",
      "78           identify\n",
      "79          important\n",
      "80              words\n",
      "81                 in\n",
      "82                  a\n",
      "83           document\n",
      "84           relative\n",
      "85                 to\n",
      "86                  a\n",
      "87         collection\n",
      "88                 of\n",
      "89          documents\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "tokens = word_tokenize(document)\n",
    "tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "df = pd.DataFrame({'tokenized_word:': tokens})\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"\\nProcessed DataFrame:\\n\")\n",
    "print(df.to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c821c0c-bf18-44dd-ba79-87d23d5f5751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame (POS Tagging):\n",
      "\n",
      "       Tokenized Word POS Tag\n",
      "0                Text      NN\n",
      "1           analytics     NNS\n",
      "2                  is     VBZ\n",
      "3                 the      DT\n",
      "4             process      NN\n",
      "5                  of      IN\n",
      "6          extracting     VBG\n",
      "7          meaningful      JJ\n",
      "8            insights     NNS\n",
      "9                from      IN\n",
      "10            textual      JJ\n",
      "11               data     NNS\n",
      "12                 It     PRP\n",
      "13           involves     VBZ\n",
      "14            various      JJ\n",
      "15      preprocessing     VBG\n",
      "16         techniques     NNS\n",
      "17               such      JJ\n",
      "18                 as      IN\n",
      "19       tokenization      NN\n",
      "20          stop-word      NN\n",
      "21            removal      NN\n",
      "22           stemming      NN\n",
      "23                and      CC\n",
      "24      lemmatization      NN\n",
      "25              These      DT\n",
      "26         techniques     NNS\n",
      "27               help     VBP\n",
      "28                 in      IN\n",
      "29        structuring     VBG\n",
      "30       unstructured      JJ\n",
      "31               text      NN\n",
      "32             making     VBG\n",
      "33                 it     PRP\n",
      "34             easier     JJR\n",
      "35                 to      TO\n",
      "36            analyze      VB\n",
      "37            Natural     NNP\n",
      "38           Language     NNP\n",
      "39         Processing     NNP\n",
      "40                NLP     NNP\n",
      "41              plays     VBZ\n",
      "42                  a      DT\n",
      "43        significant      JJ\n",
      "44               role      NN\n",
      "45                 in      IN\n",
      "46               text      JJ\n",
      "47          analytics     NNS\n",
      "48                 by      IN\n",
      "49           enabling     VBG\n",
      "50           machines     NNS\n",
      "51                 to      TO\n",
      "52         understand      VB\n",
      "53                and      CC\n",
      "54            process      VB\n",
      "55              human      JJ\n",
      "56           language      NN\n",
      "57       Applications     NNS\n",
      "58                 of      IN\n",
      "59               text      JJ\n",
      "60          analytics     NNS\n",
      "61            include     VBP\n",
      "62          sentiment      JJ\n",
      "63           analysis      NN\n",
      "64           document      NN\n",
      "65     classification      NN\n",
      "66                and      CC\n",
      "67        information      NN\n",
      "68          retrieval      NN\n",
      "69                 By      IN\n",
      "70          computing     VBG\n",
      "71               Term     NNP\n",
      "72  Frequency-Inverse     NNP\n",
      "73           Document     NNP\n",
      "74          Frequency     NNP\n",
      "75             TF-IDF     NNP\n",
      "76                 we     PRP\n",
      "77                can      MD\n",
      "78           identify      VB\n",
      "79          important      JJ\n",
      "80              words     NNS\n",
      "81                 in      IN\n",
      "82                  a      DT\n",
      "83           document      NN\n",
      "84           relative      NN\n",
      "85                 to      TO\n",
      "86                  a      DT\n",
      "87         collection      NN\n",
      "88                 of      IN\n",
      "89          documents     NNS\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tokens = word_tokenize(document)\n",
    "tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "df = pd.DataFrame(pos_tags, columns=['Tokenized Word', 'POS Tag'])\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(\"\\nProcessed DataFrame (POS Tagging):\\n\")\n",
    "print(df.to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25ea0e84-8e91-4325-a893-5c355d53971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation of the Uploaded Document:\n",
      "\n",
      "                   0     1     2     3     4     5\n",
      "analysis       0.000 0.000 0.000 0.000 0.325 0.000\n",
      "analytics      0.219 0.000 0.000 0.165 0.225 0.000\n",
      "analyze        0.000 0.000 0.322 0.000 0.000 0.000\n",
      "and            0.000 0.193 0.000 0.165 0.225 0.000\n",
      "applications   0.000 0.000 0.000 0.000 0.325 0.000\n",
      "as             0.000 0.279 0.000 0.000 0.000 0.000\n",
      "by             0.000 0.000 0.000 0.195 0.000 0.176\n",
      "can            0.000 0.000 0.000 0.000 0.000 0.214\n",
      "classification 0.000 0.000 0.000 0.000 0.325 0.000\n",
      "collection     0.000 0.000 0.000 0.000 0.000 0.214\n",
      "computing      0.000 0.000 0.000 0.000 0.000 0.214\n",
      "data           0.316 0.000 0.000 0.000 0.000 0.000\n",
      "document       0.000 0.000 0.000 0.000 0.267 0.351\n",
      "documents      0.000 0.000 0.000 0.000 0.000 0.214\n",
      "easier         0.000 0.000 0.322 0.000 0.000 0.000\n",
      "enabling       0.000 0.000 0.000 0.238 0.000 0.000\n",
      "extracting     0.316 0.000 0.000 0.000 0.000 0.000\n",
      "frequency      0.000 0.000 0.000 0.000 0.000 0.428\n",
      "from           0.316 0.000 0.000 0.000 0.000 0.000\n",
      "help           0.000 0.000 0.322 0.000 0.000 0.000\n",
      "human          0.000 0.000 0.000 0.238 0.000 0.000\n",
      "identify       0.000 0.000 0.000 0.000 0.000 0.214\n",
      "idf            0.000 0.000 0.000 0.000 0.000 0.214\n",
      "important      0.000 0.000 0.000 0.000 0.000 0.214\n",
      "in             0.000 0.000 0.223 0.165 0.000 0.148\n",
      "include        0.000 0.000 0.000 0.000 0.325 0.000\n",
      "information    0.000 0.000 0.000 0.000 0.325 0.000\n",
      "insights       0.316 0.000 0.000 0.000 0.000 0.000\n",
      "inverse        0.000 0.000 0.000 0.000 0.000 0.214\n",
      "involves       0.000 0.279 0.000 0.000 0.000 0.000\n",
      "is             0.316 0.000 0.000 0.000 0.000 0.000\n",
      "it             0.000 0.229 0.264 0.000 0.000 0.000\n",
      "language       0.000 0.000 0.000 0.477 0.000 0.000\n",
      "lemmatization  0.000 0.279 0.000 0.000 0.000 0.000\n",
      "machines       0.000 0.000 0.000 0.238 0.000 0.000\n",
      "making         0.000 0.000 0.322 0.000 0.000 0.000\n",
      "meaningful     0.316 0.000 0.000 0.000 0.000 0.000\n",
      "natural        0.000 0.000 0.000 0.238 0.000 0.000\n",
      "nlp            0.000 0.000 0.000 0.238 0.000 0.000\n",
      "of             0.219 0.000 0.000 0.000 0.225 0.148\n",
      "plays          0.000 0.000 0.000 0.238 0.000 0.000\n",
      "preprocessing  0.000 0.279 0.000 0.000 0.000 0.000\n",
      "process        0.260 0.000 0.000 0.195 0.000 0.000\n",
      "processing     0.000 0.000 0.000 0.238 0.000 0.000\n",
      "relative       0.000 0.000 0.000 0.000 0.000 0.214\n",
      "removal        0.000 0.279 0.000 0.000 0.000 0.000\n",
      "retrieval      0.000 0.000 0.000 0.000 0.325 0.000\n",
      "role           0.000 0.000 0.000 0.238 0.000 0.000\n",
      "sentiment      0.000 0.000 0.000 0.000 0.325 0.000\n",
      "significant    0.000 0.000 0.000 0.238 0.000 0.000\n",
      "stemming       0.000 0.279 0.000 0.000 0.000 0.000\n",
      "stop           0.000 0.279 0.000 0.000 0.000 0.000\n",
      "structuring    0.000 0.000 0.322 0.000 0.000 0.000\n",
      "such           0.000 0.279 0.000 0.000 0.000 0.000\n",
      "techniques     0.000 0.229 0.264 0.000 0.000 0.000\n",
      "term           0.000 0.000 0.000 0.000 0.000 0.214\n",
      "text           0.188 0.000 0.191 0.141 0.193 0.000\n",
      "textual        0.316 0.000 0.000 0.000 0.000 0.000\n",
      "tf             0.000 0.000 0.000 0.000 0.000 0.214\n",
      "the            0.316 0.000 0.000 0.000 0.000 0.000\n",
      "these          0.000 0.000 0.322 0.000 0.000 0.000\n",
      "to             0.000 0.000 0.223 0.165 0.000 0.148\n",
      "tokenization   0.000 0.279 0.000 0.000 0.000 0.000\n",
      "understand     0.000 0.000 0.000 0.238 0.000 0.000\n",
      "unstructured   0.000 0.000 0.322 0.000 0.000 0.000\n",
      "various        0.000 0.279 0.000 0.000 0.000 0.000\n",
      "we             0.000 0.000 0.000 0.000 0.000 0.214\n",
      "word           0.000 0.279 0.000 0.000 0.000 0.000\n",
      "words          0.000 0.000 0.000 0.000 0.000 0.214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = sent_tokenize(document)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "print(\"\\nTF-IDF Representation of the Uploaded Document:\\n\")\n",
    "print(df_tfidf.round(3).T.to_string(index=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
